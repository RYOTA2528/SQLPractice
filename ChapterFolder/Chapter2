
(代表的なビッグデータの処理ツール)																									
①PostgreSQL																									
・オープンソースのRDBです。標準SQLへの準拠率が高い。ウィンドウ関数やCTEなど、分析に必須となる関数が多く収容されてる。																									
・特有の拡張機能が多く実装されてる。																									
・小規模な分析やSQL学習を目的とする利用に最適。ローカルで比較的軽量で動作するPostgreSQLの使用がおすすめ

②Apache Hive																									
・HDFSと呼ばれる分散ファイルシステム上のデータを、SQLライクなインターフェースで簡単に処理できる																									
・バラバラに配置されたデータを並列に処理する手法はMapReduce																									
・HDFSとMapReduceのアーキテクチャを実装したシステムが初期のApache Hadoop、HiveはHadoopエコシステムの一部																									
・（デメリット）	
　           　・PostgreSQLと比較するとあくまでファイルベースのシステムある。特定レコードだけの更新・削除は難しく、インデックスも基本的には存在しない。クエリ発行時にファイル全件を走査することになる。																								
	             ・あくまでもスループットを高めるためのものであるためレイテンシの低い処理を求められるケースにむいてない。																								
・（メリット）
               ・クエリ実行時に動的にデータの定義を与えられる。(ex)『Japan Tokyo Minato-ku』のデータに対して、『国・都道府県・市区町村』の3つのカラムデータとして取り扱うのではでなく、1つのとして扱うことが可能です。これにより『具体的に何に使うかわからないがとりあえず保存しておきたいデータ』をHDFS上に蓄積し、必要になったタイミングに応じて、動的にスキーマに定義できる																								
	             ・データ分析のための豊富なUDFを活用でき、SQLだけでは実現が難しい文字列処理なども簡単に実現できる。→Javaで実装可能！	
	
③Amazon Redshift	　																								
・AWSで提供される並列RDS																									
・行単位での更新・削除も可能。																									
（メリット）	・RDBでは扱えない大量データに対して、インタラクティブにクエリを発行したい場合に効果的。HiveではMapReduceによる分散処理を実行する場合、どんな軽いジョブでも10秒以上かかる。Redshiftでは最短数ミリ秒で実行可能。																								
	　　　　　　・利用時間に応じて課金される。																								
（デメリット）	
　　　　　　　・パフォーマンスチューニングや金額を抑えたい運用を行いたい場合、最適なノード数やノードスペックの見積もり、インスタンスの起動・終了の管理などが必要となりある程度の専門的な知識が必要となる。																								
	            ・PostgreSQLとのアーキテクチャ的な違いとして、Redshiftは列指向のストレージ（レコードごとではなく列ごとに保存）である点が特徴的であり、テーブル設計やクエリ実行時に通常のRDBとは異なる発想も必要になる。																								
	            ・このアーキテクチャによりデータ圧縮率を向上させたり、クエリ実行時にディスクI/Oを削除できます。																								
              ・分析に必要なデータを全て一つのテーブル列に追加する形式をとるケースも多々あり、クエリ実行の際『SELECT*』などの全ての列を取得するクエリはパフォーマンスを低下させることにつながるため
  　             必ず、『必要な列に絞って』クエリ実行する必要がある。
  　
④Google BigQuery
・ビッグデータ解析のためのGoogle上のクラウドサービス。
・SQLライクなクエリ言語。主に使用するのが『スタンダードSQL』で、CTEや相関サブクエリに対応するなど、一般的な構文でクエリを記述可能となっています。
（メリット）・Redshiftと違い、自分で計算ノードのインスタンスを管理する必要がなく、利用時間ではなく読み込むだデータ量で課金される点が特徴的。
　　　　　　・有料版GoogleAnlticsのデータをBigQueryで取り扱い可能であり、Google Cloud Storagesから手軽にデータをロードできるなど、他のサービスとの親和性が高い。
(デメリット)
　　　　　　・読み込んだデータ量で課金される
　　　　　　・データをロードする必要がある場合には、データ量に応じた金額が発生するため、クエリ発行時のデータを抑えるため、一緒に読み込まれやすいデータの範囲ごとに
　　　　　　　テーブルを分割する、必要なカラムだけ選択してSELECTするなどテクニカルが必要。
　　　　　　・列指向のアーキテクチャです。
　　　　　　
⑤SparkSQL
・MapReduceに続く分散処理フレームワークであるApacha Sparkの機能の内、SQLインターフェースをに関連する機能を表す言葉。
・オープンソースのフレームワークである為、無料で利用できる。非常に速い。機械学習やグラフ処理、リアルタイムストリーミング処理など、さまざまな処理を手軽に分散処理できる機能を提供。
（メリット）
						・ビッグデータ活用に関連するほとんどの処理をオールインワンで実現できる。
						・SparkはインターフェースとしてSQLだけでなく、PythonやScala、Java、Rなどのプログラミング言語に対応。データのインポートやエクスポート機能も豊富にジッソyされてる。
						・DataFramesのAPIが標準であり、どの言語を用いても自動的に最適化される。
						・DataFramesのAPIはSQLに似た宣言的な構文でデータを操作できるだけでなく、手続き型のプログラミングに近い方法でプログラムを実装できる。
						　(ex)例えば、中間データを逐一標準出力に書き出して、途中経過を確認しながら処理を実行できたり、データ処理を細かいモジュールに分割して単体テストやアノテーション
						　　　を実装できるメリットに加え、機械学習や統計処理、グラフ処理などSQLだけでは難しい処理を統合したりリアルタイムに受け取ったデータをストリーム処理するなど多種多様なメリットがある。



　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　【データについて】
（注意）
・業務前にどんな種類のデータを持ち合わせるかを把握することで、可能なことと不可能なことの見当がつくため、手戻りなくスムーズに業務を進められる。
（データの種類）
『業務データ』と『ログデータ』について
		
　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　『業務データ』
		→サービス・システムを運用する目的で構築されたデータベースに存在するデータ
		→ほとんどが『更新型』のデータ(ex)商品データの更新
		→連続したデータの途中で何らかの不具合が生じてもロールバック処理をし、処理を取り消せる。正常に処理できたものが保存される仕組みを持つ（トランザクション機能）、その為
		、正確な値であることが求められる。(ex)売り上げに関するレポートはこの業務レポートが使われる。
		→データの冗長性を控除し、データの整合性を保ちやすいようデータを保存する（正規化）。その為、1つのテーブルをさんしょうするだけではそのデータが誰の行動かなんの購入かまでは把握できない。
		　ER図に書き起こしたうえでデータの整合性を保ちやすいよう正規化する。複数のテーブルの結合が大切
				業務データは2種類
					『トランザクションデータ』
					(ex)購入データ・口コミデータ・ゲームのプレイデータなど。サービス・システムを通じてユーザーの行動を記録したデータを示す。データの日付・時刻・マスターデータの怪異ID,
					　　商品ID、数量、価格帯等が含まれることが多い。
					　　これらのデータは会員IDと商品IDで格納されてるケースが多く、会員の性別や居住地、商品のカテゴリなどをレポートにして扱うことはできない。
				　『マスターデータ』
				　(ex)都道府県データ、カテゴリデータ、商品マスタなど、サービス・システムが定義するデータをマスターデータと呼ぶ。会員に関する情報も会員マスタデータに分類されてる。
				　
				　つまり、トランザクションデータの商品IDとマスターデータを突き合わせることで、商品名や商品カテゴリーを、販売日などが明らかになりレポート業務の幅が広がる。
				　【注意】：トランザクションデータのみでは、分析範囲が限定される。含まれるマスターデータはレポート業務の前に一通り、そろってることを確認すること！！
				　
		
			『業務データを蓄積する方法』
				（前略）業務データを分析用の環境に転送するには『Apache Spoop』などRDBからデータロードを行うシステムを利用する！
　　　その１，
　　　　全レコードを入れ替える。（マスターデータのように更新・削除の少ない、日次でデータが積みあがるものでない場合）(ex)商品カテゴリー、都道府県
　　　その２,
　　　　全レコードのスナップショットを日付ごとに保存する。
　　　　(ex)2022年1月1日時点のユーザマスタ、2022年1月2日時点のユーザマスタと全レコードを日付で積み上げることで影響を少なくする。
　　　その３,
　　　  前日からの差分のレコードを積み上げる。
　　　  トランザクションデータの中で、更新・削除がなく追加のみされるテーブルは全レコードを入れ替えても構わない。ただし、データ転送量が膨れ上がり処理時間を要するため、
　　　  前日からの差分データにみを積み上げるといい。
　　　 
　　　 『業務データの扱い』
　　　 （注意）
　　　 トランザクション機能→データは正しい→SQLが間違ってない限りデータは正しい。
　　　 ログデータは送信方法によっては欠損する可能性もあるため、『正しい値』が求められるシーンでは業務データを利用。
　　　その１,
　　　  サービスにおける訪問回数・ページビュー・回遊などのデータは分析できない。（これらはログデータで管理）
　　　その２,
　　　  更新型故に、抽出結果が変わる可能性があることに注意。データの蓄積を検討する必要あり。
　　　  どうしても更新型の影響を抑えきれず、無視できない誤差になるなら『抽出時点の情報を元に』あらかじめ共有し、レポートを作成・提出する			　
	
				　
																																		　『ログデータ』
			→集計・分析を主な用途とした設計されてるデータ
			→サイトに特定のタグを埋め込んで送信するデータ
			→取得方法によって、ログデータの精度が異なる。
			→特定の行動に対して、サーバ側で出力するデータ
			（ex）時刻、ユーザエージェント、IP、URL、リファラー、Cookieなどの情報を保存する。
					これらは『追記型データ』です。また、追記型故に、過去のデータを更新しない。
											：(ex)価格が変更されたり、ユーザー情報が更新されてもすでに出力したデータが書き換えられることはない。
											
			『ログデータを蓄積する方法』
			　（前略）取り扱うデータがどんなログデータか把握→以降のデータの扱いを検討→レポートで説明する範囲を明確にすること。
		その１,
	    タグ、SDKを埋め込んで、ユーザ端末からデータを送信する。
	  その２,
	    サーバ側でデータを取得し出力する（サーバ側）
	    
	  　『ログデータの扱い』
	  　その１,
	  　	主にサイトの訪問回数、ページビュー、回遊状況を集計・分析するために使用する。
	  　	　業務データでは管理しきれない閲覧ページ、リファラー、ユーザーエージェントなどを保存することは可能。アクセス解析ツールで確認する訪問回数やページビュー、アクション数、
	  　	　デバイス別訪問数などの指標を求める際に使用する。
	  　その２,
	  　  出力時点の情報で分析可能だが、最新の状況を考慮した分析にはむかない
	  　  　ログ出力以降のデータ変更を適用して分析するには、データの加工が必要となる(ex)商品カテゴリーの見直し。ユーザの居住地変更など
	  　その３,
	  　　追記型のデータゆえ、抽出結果変わる状況は少ない。
	  　　　ログデータは更新・削除が発生しないので、期間を指定して集計したクエリの結果が変わることはない。
	  　その４,
	  　  データの正確さについては、業務データより劣る。
	  　  　ログの取得方法によっては欠損するユーザーが発生したりクローラーのログが混入する可能性があるため正確なあたいを求める目的にはむいてない
	  　  　
	  　  　
	  　『業務データ・ログデータの具体的な活用方法』
	  　		→2つのデータを横断的に活用することで新しい価値が生まれる。
	  　　　　　
	  　　　　　『業務データ』
	  　	　　　		→売上額の推移やどの商品が人気なのか明らかになればユーザーにもっと周知して購入してもらうことが可能。
	  　	　　　		→商品による季節変動もあり、ある時期になれば売れるものが過去の傾向からつかめれば、『その時期には表示額を変えたり、特集を組むなどして商品の露出を高める』
	  　	　　　		(ex)3ー4月は沖縄で売れ・6-7月は本州で売れるなどの傾向をつかみ在庫をコントロール
	  　	　　　		　　季節が冬の場合は、北海道での売れ行きを確認して本州・沖縄へ反映させる。
	  　	　　　		　　
	  　	　　　 『ログデータ』
	  　	　　　 　 →多くのアクセス解析ツールはビーコン形式でログを送信する機構を埋め込みそのログを利用して集計した結果をレポートとして提供。
	  　	　　　 　 ※基本的にはページビュー、アクションそのデータに含まれる値（ユーザーエージェントなど）を集計し表示※
	  　	　　　 　 →ECサイト・ニュースサイト・コミュニティサイトなど、多種多様なサービスがありつつもアクセス解析ツールのレポートはどのサービスでの共通して利用可能
	  　	　　　 　 →レポートのフォーマットや計算結果を自ら考えて集計するにはその機能が実装されない限りは困難。
	  　	　　　 　 →ビッグデータ基盤なら自ら欲しいレポートを自由に定義でき、データ収集から加工、集計を自由に行えるので、アクセス解析ツールの制限をうけることはない。
	  　	　　　 　 →主にWEBサイトの行動を記録し活用する。オフラインでのデータも利用できる。POSデータや来店成約に至ったデータ、対応履歴など様々なデータが考えられる
	  　	　　　 　 
	  『業務データとログデータの横断的な主計の先にあるもの』
	  　　※WEBサイトでの行動がオフラインの行動にどう影響するのか、調査が可能※
	  　　  (ex)
	  　　  　特定のメディアや広告から遷移したユーザは制約に至る可能性が高いことがわかれば、そのメディアへや広告への出稿額をあげることで多くの成約を得ることができる
	  　	　　来店前に自社ＷＥＢサイトでのどの商品を閲覧していたか履歴がわかれば接客の際の対応を変えることができたりと、メディアの成約率をあげるためのデータ分析の重要性を理解した。　　 　 
	  　
	  『どのようにすれば、価値のあるデータ分析ができるか？　活用事例一覧』
	 　 ①目標管理（目標を管理し計画立ててサービス・組織の成長を促す）
	  　　売上・アクセス数・ユーザー数など、サービスが目指す目標に対して、現時点の進捗はどれくらいかを把握し不足していれば達成できる施策を考える。
	  　②サービス改善（ユーザーの行動から傾向を発見し、売り上げやサービスの改善を図る）
	  　　ユーザーインタビューを都度実施は時間も費用も掛かる。その為、大量のデータの中からユーザー傾向を探り売上向上やサービス改善が可能なことがデータの価値となる。
	  　③未来予測（過去の傾向から未来の行動を予測する）
	  　　(ex)
	  　　 季節や地理的な特性を生かす、将来の在庫を管理する、WEBサイトで特定の行動をするユーザは、近い将来サービスを退会するため事前の対策を講じる、レコメンド
	  　　 
	  　　これらがビッグデータが注目を浴びる所似と考えられる。このようなことが可能になるデータがどんなデータであるか検討しデータを収集する必要